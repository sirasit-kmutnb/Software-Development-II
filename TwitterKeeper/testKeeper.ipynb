{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#28thHappyENGFADAY', \"Happy Valentine's Day\", 'Bright', '#MoonlightChickenEP3', '#adidascentralworld', 'Coming Soon', 'SUPERSTAR JENNIE IN CALVINS', '#AIS5GxBamBam', '#zeenunewmysweetheart', '#BUILDINBLACK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 520.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from Twitter_keeper import PullTweetsData\n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Authenticate to Twitter\n",
    "api_key = os.getenv('API_KEY')\n",
    "api_key_secret = os.getenv('API_KEY_SECRET')\n",
    "access_token = os.getenv('ACCESS_TOKEN')\n",
    "access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')\n",
    "\n",
    "Puller = PullTweetsData()\n",
    "Puller.getAccessToAPI(api_key, api_key_secret)\n",
    "Puller.setUserAuthentication(access_token, access_token_secret)\n",
    "Puller.getTwitterAPI()\n",
    "Puller.connectToDB(\"twitter_keeper\", \"tweets_JP\")\n",
    "\n",
    "TH_Bangkok = 1225448\n",
    "JP_Tokyo = 1118370\n",
    "\n",
    "\n",
    "trends = Puller._PullTweetsData__api.get_place_trends(TH_Bangkok)\n",
    "top50 = trends[0]['trends']\n",
    "new_list = [d for d in top50 if d.get('tweet_volume') != None]\n",
    "sorted_list = sorted(new_list, key=lambda x: x['tweet_volume'], reverse=True)\n",
    "top10 = sorted_list[0:10]\n",
    "names = [d['name'] for d in top10]\n",
    "print(names)\n",
    "pd.DataFrame(top10)\n",
    "\n",
    "# t1 = Thread(target=Puller.pullTweets, args=(\"#Valentine\", 100))\n",
    "# t1.start()\n",
    "\n",
    "for i in tqdm(names):\n",
    "    # print(names[i])\n",
    "    t1 = Thread(target=Puller.pullTweets, args=(i, 10))\n",
    "    t1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Twitter_keeper import PullTweetsData\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import emoji\n",
    "import numpy as np\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "import pandas as pd\n",
    "\n",
    "class FindTopWord(PullTweetsData):\n",
    "\n",
    "    def tokenize(self,d):  \n",
    "        result = d.split(\"/\")\n",
    "        result = list(filter(None, result))\n",
    "        return result\n",
    "\n",
    "    def prepared_Text(self,text_list):\n",
    "        new_text = []\n",
    "        for text in text_list:\n",
    "            new_text.append(self.preprocessText(text))\n",
    "        return new_text\n",
    "        \n",
    "    def MostWordFinder(self,tweets_list):\n",
    "        vectorizer = CountVectorizer(tokenizer=self.tokenize)\n",
    "        transformed_data = vectorizer.fit_transform(tweets_list)\n",
    "        keyword_df1 = pd.DataFrame(columns = ['word', 'count'])\n",
    "        keyword_df1['word'] = vectorizer.get_feature_names_out()\n",
    "        print(vectorizer.get_feature_names_out())\n",
    "        keyword_df1['count'] = np.ravel(transformed_data.sum(axis=0))   \n",
    "        keyword_df1.sort_values(by=['count'], ascending=False).head(10)\n",
    "        return keyword_df1\n",
    "\n",
    "\n",
    "\n",
    "class SentimentAnalyze(PullTweetsData):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__df_train = pd.read_csv(\"general-amy.csv\")\n",
    "        self.__vectorizer = CountVectorizer()\n",
    "        self.__model = MultinomialNB()\n",
    "\n",
    "    def preprocess_train_text(self,text):\n",
    "        text = self.removeLink(text)\n",
    "        text = self.removeEmoji(text)\n",
    "        text = self.removeSpecialChar(text)\n",
    "        final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', \"ๆ\", \"ฯ\"))\n",
    "        final = word_tokenize(final, engine=\"newmm\")\n",
    "        final = \" \".join(word for word in final)\n",
    "        # final = \" \".join(word for word in final.split() if word.lower not in thai_stopwords())\n",
    "        return final\n",
    "        # tokens = word_tokenize(text, engine=\"newmm\")\n",
    "        # result = [word for word in tokens if word not in list(\n",
    "        #         thai_stopwords()) and \" \" not in word]\n",
    "        # return \" \".join(result).rstrip()\n",
    "\n",
    "    def run_prep_train(self):\n",
    "        self.__df_train['text'] = self.__df_train['text'].apply(self.preprocess_train_text)\n",
    "\n",
    "    def split_training(self):\n",
    "        self.X = self.__vectorizer.fit_transform(self.__df_train['text'])\n",
    "        self.y = self.__df_train['sentiment']\n",
    "        # Split the data into training and testing sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "\n",
    "    def training_model(self):\n",
    "        self.run_prep_train()\n",
    "        self.split_training()\n",
    "        # Train the model\n",
    "        self.__model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def evaluating_model(self):\n",
    "        #Evaluate the model on the test data\n",
    "        y_pred = self.__model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        return accuracy\n",
    "\n",
    "    def sentiment_analyzer(self,text):\n",
    "        #Use the model to make predictions on new data\n",
    "        new_text = self.__vectorizer.transform([text])\n",
    "        new_bag_of_word = self.__vectorizer.transform(pd.Series([self.preprocess_train_text(text)]))\n",
    "        # print(new_bag_of_word)\n",
    "        new_pred = self.__model.predict(new_bag_of_word)\n",
    "        return new_pred[0]\n",
    "\n",
    "class main():\n",
    "    def __init__(self):\n",
    "        self.find_top_word = FindTopWord()\n",
    "        self.sentiment_analyze = SentimentAnalyze()\n",
    "        self.pull_tweets = PullTweetsData()\n",
    "\n",
    "    def load_sample_tweets(self,author=\"\",keyword=\"\",hashtag=\"\",location=\"\",text=\"\",fromTime=\"\",toTime=\"\"):\n",
    "        self.pull_tweets.connectToDB(\"twitter_keeper\",\"tweets\")\n",
    "        return self.pull_tweets.find_multi(author,keyword,hashtag,location,text,fromTime,toTime)\n",
    "\n",
    "    def tweets_find_top_word(self,author=\"\",keyword=\"\",hashtag=\"\",location=\"\",text=\"\",fromTime=\"\",toTime=\"\"):\n",
    "        tweets_list = self.pull_tweets.prepared_Text(self.load_sample_tweets(author,keyword,hashtag,location,text,fromTime,toTime))\n",
    "        return self.find_top_word.MostWordFinder(tweets_list)\n",
    "\n",
    "    def tweets_sentiment_analyzer(self,author=\"\",keyword=\"\",hashtag=\"\",location=\"\",text=\"\",fromTime=\"\",toTime=\"\"):\n",
    "        self.sentiment_analyze.training_model()\n",
    "        acc = self.sentiment_analyze.evaluating_model()\n",
    "        tweets_list = self.load_sample_tweets(author,keyword,hashtag,location,text,fromTime,toTime)\n",
    "        df = pd.DataFrame({'text':[],'sentiment':[]})\n",
    "        for tweet in tqdm(tweets_list):\n",
    "            sentiment = self.sentiment_analyze.sentiment_analyzer(tweet['text'])\n",
    "            df = pd.concat([df,pd.DataFrame(pd.Series([tweet['text'],sentiment], index=df.columns)).T],ignore_index=True)\n",
    "        return df\n",
    "\n",
    "TweetsCounter = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1268.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ในสตรีมพี่เอกบอกว่าให้พี่กายแคปหลักฐานที่คนในท...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ถ้าพี่เอกติดอยู่ใน Borderland พี่เอกคงเป็น Kin...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>อยากกราบความมีสมาธิของพี่เอกจริงๆ จำรหัสได้ทั้...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ถ้าพี่เอกติดอยู่ใน Borderland พี่เอกคงเป็น Kin...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ถ้าพี่เอกติดอยู่ใน Borderland พี่เอกคงเป็น Kin...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>ผมที่รอวันนี้มานาน วันที่พี่เอกสู้คน\\n#ใส่นัวแ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ฝากช่วยรีหน่อยนะคะ คุณยายขายน้ำพริกกับทุเรียนท...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>🧒: ขอคืนได้มั้ยคะ?\\nปะเต้🐍: ไม่! อยากได้ก็มาแย...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>🐡: เดี๋ยว แล้วไปที่ไหนอะ\\n🐻: เอ้า มีใครรู้รหัส...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>🧒: ขอคืนได้มั้ยคะ?\\nปะเต้🐍: ไม่! อยากได้ก็มาแย...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text sentiment\n",
       "0    ในสตรีมพี่เอกบอกว่าให้พี่กายแคปหลักฐานที่คนในท...       pos\n",
       "1    ถ้าพี่เอกติดอยู่ใน Borderland พี่เอกคงเป็น Kin...       pos\n",
       "2    อยากกราบความมีสมาธิของพี่เอกจริงๆ จำรหัสได้ทั้...       pos\n",
       "3    ถ้าพี่เอกติดอยู่ใน Borderland พี่เอกคงเป็น Kin...       pos\n",
       "4    ถ้าพี่เอกติดอยู่ใน Borderland พี่เอกคงเป็น Kin...       pos\n",
       "..                                                 ...       ...\n",
       "995  ผมที่รอวันนี้มานาน วันที่พี่เอกสู้คน\\n#ใส่นัวแ...       pos\n",
       "996  ฝากช่วยรีหน่อยนะคะ คุณยายขายน้ำพริกกับทุเรียนท...       pos\n",
       "997  🧒: ขอคืนได้มั้ยคะ?\\nปะเต้🐍: ไม่! อยากได้ก็มาแย...       pos\n",
       "998  🐡: เดี๋ยว แล้วไปที่ไหนอะ\\n🐻: เอ้า มีใครรู้รหัส...       pos\n",
       "999  🧒: ขอคืนได้มั้ยคะ?\\nปะเต้🐍: ไม่! อยากได้ก็มาแย...       pos\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display,HTML\n",
    "\n",
    "df = TweetsCounter.tweets_sentiment_analyzer(text=\"\",keyword=\"#ใส่นัวแฟมิลี่\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
